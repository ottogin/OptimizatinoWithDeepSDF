{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import (NNConv, GMMConv, GraphConv, Set2Set)\n",
    "from torch_geometric.nn import (SplineConv, graclus, max_pool, max_pool_x, global_mean_pool)\n",
    "\n",
    "#from neuralnet_pytorch.metrics import chamfer_loss\n",
    "\n",
    "import trimesh\n",
    "\n",
    "from visualization_utils import plot_mesh_3d\n",
    "\n",
    "import deep_sdf\n",
    "import deep_sdf.workspace as ws\n",
    "from models import *\n",
    "from datasets import *\n",
    "from custom_utils import *\n",
    "\n",
    "from sklearn.neighbors import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeAvgTransform():\n",
    "    objects = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(\"/cvlabdata2/home/artem/Data/cars_remeshed_dsdf/transforms/\"):\n",
    "        objects += [os.path.join(dirpath, file) for file in filenames if file[-4:] == '.npy']\n",
    "    \n",
    "    matricies = []\n",
    "    for obj in objects:\n",
    "        matricies.append(np.load(obj))\n",
    "    \n",
    "    return np.mean(np.array(matricies), axis=0)\n",
    "\n",
    "AvgTransform = computeAvgTransform()\n",
    "\n",
    "def transformPoints(points, matrix):\n",
    "    matrix = torch.cuda.FloatTensor(matrix)\n",
    "    column = torch.zeros((len(points), 1), device=\"cuda:0\") + 1\n",
    "    stacked = torch.cat([points, column], dim=1)\n",
    "    transformed = torch.matmul(matrix, stacked.T).T[:, :3]\n",
    "    return transformed\n",
    "\n",
    "def make_mesh_from_points(points, ply_mesh):\n",
    "    transformed_points = transformPoints(points, AvgTransform)\n",
    "    \n",
    "    edges = trimesh.geometry.faces_to_edges(ply_mesh['face']['vertex_indices'])\n",
    "    np_points = transformed_points.cpu().detach().numpy()\n",
    "    edge_attr = [np_points[a] - np_points[b] for a, b in edges]\n",
    "\n",
    "    data = torch_geometric.data.Data(x   = transformed_points, \n",
    "                                     pos = transformed_points, \n",
    "                                     face = torch.tensor(ply_mesh['face']['vertex_indices'], \n",
    "                                                         dtype=torch.long).to('cuda:0').t(),\n",
    "                                     edge_attr=torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "                                     edge_index=torch.tensor(edges, dtype=torch.long).t().contiguous().to('cuda:0'))\n",
    "    return data\n",
    "\n",
    "\n",
    "def compute_volume(vertices_in, faces_in):\n",
    "\n",
    "    bs, nv = vertices_in.shape[:2]\n",
    "    bs, nf = faces_in.shape[:2]\n",
    "    device = vertices_in.device\n",
    "    faces = faces_in + (torch.arange(bs, dtype=torch.int32).to(device) * nv)[:, None, None]\n",
    "    vertices = vertices_in #.reshape((bs * nv / 2, 3))\n",
    "    # pytorch only supports long and byte tensors for indexing\n",
    "    face_coordinates = vertices[faces.long()].squeeze(0)\n",
    "\n",
    "    x1y2z3 = face_coordinates[:,0,0]*face_coordinates[:,1,1]*face_coordinates[:,2,2]\n",
    "    x2y1z3 = face_coordinates[:,1,0]*face_coordinates[:,0,1]*face_coordinates[:,2,2]\n",
    "    x3y1z2 = face_coordinates[:,2,0]*face_coordinates[:,0,1]*face_coordinates[:,1,2]\n",
    "    x1y3z2 = face_coordinates[:,0,0]*face_coordinates[:,2,1]*face_coordinates[:,1,2]\n",
    "    x2y3z1 = face_coordinates[:,1,0]*face_coordinates[:,2,1]*face_coordinates[:,0,2]\n",
    "    x3y2z1 = face_coordinates[:,2,0]*face_coordinates[:,1,1]*face_coordinates[:,0,2]\n",
    "\n",
    "    V = x1y2z3 - x2y1z3 + x3y1z2 - x1y3z2 + x2y3z1 - x3y2z1\n",
    "    return 1.0/6.0*torch.abs(torch.sum(V))\n",
    "\n",
    "\n",
    "def boundsLoss(points, box=[(-1, 1, 0)]):\n",
    "    loss = 0\n",
    "    for l, r, i in box:\n",
    "        loss +=  torch.mean(F.relu(-points[:, i] + l))  \\\n",
    "               + torch.mean(F.relu( points[:, i] - r))\n",
    "    return loss\n",
    "\n",
    "def innerBoundsLoss(points, r=1, center=(0, 0, 0)):\n",
    "    radiuses = torch.sum( (points - torch.Tensor(center).to('cuda:0')) ** 2 , dim=1)\n",
    "    return torch.sum(F.relu(r - radiuses))\n",
    "\n",
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cvlabdata2/home/artem/DeepSDF\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_instance= make_data_instance_from_stl(\n",
    "#                '/cvlabdata2/home/artem/Data/cars_remeshed_dsdf/outputs/fld/0003_0015.fld') # /cvlabdata2/home/artem/Data/cars_refined/simulated/fld/0002_0005.fld\n",
    "device = \"cuda:0\"\n",
    "\n",
    "# modelOld = SplineCNN8(3, batchnorm1=False)\n",
    "# modelOld.load_state_dict(torch.load(\"Expirements/SplineCNN8.nn\"))\n",
    "# modelOld = modelOld.to(device)\n",
    "\n",
    "model = SplineCNN8Residuals(3)\n",
    "model.load_state_dict(torch.load(\"Expirements/Networks15/normilized_full_latest.nn\"))\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205 of latent vectors, each 1 long\n"
     ]
    }
   ],
   "source": [
    "experiment_directory = \"/cvlabdata2/home/artem/DeepSDF/examples/cars_cleared/\"\n",
    "checkpoint = \"latest\"\n",
    "decoder = load_model(experiment_directory, checkpoint).to('cuda:0')\n",
    "\n",
    "latent_vectors = ws.load_latent_vectors(experiment_directory, checkpoint)\n",
    "latent_size = latent_vectors[0].size()[0]\n",
    "print(f\"{len(latent_vectors)} of latent vectors, each {latent_size} long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vertex-vise Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_shape_by_vertex(model, inp, num_iters=30, save_to_dir='Expirements/Optimization',\n",
    "                   lr=0.05, decreased_by=2, adjust_lr_every=10, verbose=None, constraint_rad=0.1, axis=0):\n",
    "\n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    model.eval()\n",
    "    data_instance = inp.clone()\n",
    "    data_instance.x.requires_grad = True\n",
    "    optimizer = torch.optim.SGD([data_instance.x], lr=lr)\n",
    "\n",
    "    meshes = []\n",
    "    lifts = []\n",
    "    lr_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cur_lr = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        local_preds = model(data_instance)\n",
    "#             # We get input as CFD mesh, but the constraints are specified in DeepSDF\n",
    "#             # We save in CFD mesh, so no need to transform before simulation\n",
    "#             points_in_deepSDF = transformPoints(data_instance.x, np.linalg.inv(AvgTransform))\n",
    "\n",
    "        #print(points_in_deepSDF)\n",
    "        loss = calculate_loss(data_instance, local_preds,\n",
    "                              axis=axis, constraint_rad=constraint_rad)\n",
    "\n",
    "        loss.backward()\n",
    "        #print('Avg grad: ', torch.mean(torch.sum(data_instance.x.grad ** 2, axis=1)))\n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(data_instance, local_preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        np.save(preds_save_path, local_preds.cpu().detach().numpy())\n",
    "\n",
    "#             data_instance.x.grad[torch.isnan(data_instance.x.grad)] = 0\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss.detach().cpu().numpy(), ' LR: ', cur_lr)\n",
    "                #plot_points_from_torch(points)\n",
    "\n",
    "        lifts.append(loss.detach().cpu().numpy())\n",
    "        lr_plot.append(cur_lr)\n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), lifts)\n",
    "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_shape_by_vertex(model, MESH_TO_OPTIMIZE, verbose=1, save_to_dir='Expirements/OptimizationPaper/NewGenVertexDragTight', \n",
    "#                          lr=0.002, adjust_lr_every=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BB Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_shape_by_scaling(model, inp, num_iters=100, save_to_dir='Expirements/Optimization',\n",
    "                   lr=0.05, decreased_by=2, adjust_lr_every=10, verbose=None, constraint_rad=0.1, axis=0):\n",
    "\n",
    "    def transform(data, scale):\n",
    "        answ = data.clone()\n",
    "        \n",
    "        answ.x =  data.x * scale[1]\n",
    "        answ.pos = answ.x\n",
    "       # answ.edge_attr = answ.x[ answ.edge_index[0] ] - answ.x[answ.edge_index[1]] \n",
    "        return answ\n",
    "    \n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    model.eval()\n",
    "    data_instance = inp.clone()\n",
    "    data_instance.x.requires_grad = True\n",
    "    scale = torch.Tensor([[0.0, 0.0, 0.0], [1.0, 1.0, 1.0]]).to(\"cuda:0\")\n",
    "    scale.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.SGD([scale], lr=lr)\n",
    "\n",
    "    meshes = []\n",
    "    lifts = []\n",
    "    lr_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        transformed_mesh = transform(data_instance, scale)\n",
    "\n",
    "        # Restriction\n",
    "        # min_dist = compute_min_distance(transformed_mesh)\n",
    "        # scale_clapped = scale * min(1, GLOBAL_MIN_DIST / min_dist)\n",
    "        #transformed_mesh = transform(data_instance, scale)\n",
    "\n",
    "        local_preds = model(transformed_mesh)\n",
    "#             points_in_deepSDF = transformPoints(transformed_mesh.x, np.linalg.inv(AvgTransform))\n",
    "        loss = calculate_loss(transformed_mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss.detach().cpu().numpy(), ' LR: ', cur_rl)\n",
    "                #plot_points_from_torch(points)\n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, local_preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        np.save(preds_save_path, local_preds.cpu().detach().numpy())\n",
    "\n",
    "        lifts.append(loss.detach().cpu().numpy())\n",
    "        lr_plot.append(cur_rl)\n",
    "\n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), lifts)\n",
    "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_shape_by_scaling(model, MESH_TO_OPTIMIZE, verbose=1, save_to_dir='Expirements/OptimizationPaper/BboxDragDiff', lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Foam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_shape_as_pierre(model, inp, num_iters=100, save_to_dir='Expirements/Optimization',\n",
    "                             lr=0.05, decreased_by=2, adjust_lr_every=10, verbose=None, \n",
    "                             constraint_rad=0.1, axis=0, regularization=1):\n",
    "\n",
    "    def transform(data, scale):\n",
    "        answ = data.clone() #scale[0]\n",
    "        answ.x = \\\n",
    "            data.x * (scale[1] + scale[2] * data.x + \\\n",
    "                      scale[3] * torch.cos(data.x[:, (2, 0, 1)] * 2 * math.pi) + \\\n",
    "                      scale[4] * torch.cos(data.x[:, (1, 2, 0)] * 2 * math.pi) + \\\n",
    "                      scale[5] * torch.sin(data.x[:, (2, 0, 1)] * 2 * math.pi) ** 2 + \\\n",
    "                      scale[5] * torch.sin(data.x[:, (1, 2, 0)] * 2 * math.pi) ** 2)\n",
    "        answ.pos = answ.x\n",
    "#         answ.edge_attr = answ.x[ answ.edge_index[0] ] - answ.x[answ.edge_index[1]] \n",
    "        return answ\n",
    "    \n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "    \n",
    "    model.eval()\n",
    "    data_instance = inp.clone()\n",
    "    data_instance.x.requires_grad = True\n",
    "    scale = torch.Tensor([[0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                          [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "                          [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]]).to(\"cuda:0\").t()\n",
    "    scale.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.SGD([scale], lr=lr)\n",
    "\n",
    "    meshes = []\n",
    "    lifts = []\n",
    "    lr_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        transformed_mesh = transform(data_instance, scale)\n",
    "\n",
    "        local_preds = model(transformed_mesh)\n",
    "#             points_in_deepSDF = transformPoints(transformed_mesh.x, np.linalg.inv(AvgTransform))\n",
    "        loss = calculate_loss(transformed_mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "\n",
    "        loss.backward()\n",
    "        l2_norm = regularization * torch.mean(scale[2:] ** 2)\n",
    "        l2_norm.backward()\n",
    "#         print(\"Grad: \", scale.grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss.detach().cpu().numpy(), ' LR: ', cur_rl)\n",
    "                #plot_points_from_torch(points)\n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, local_preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        np.save(preds_save_path, local_preds.cpu().detach().numpy())\n",
    "\n",
    "        lifts.append(loss.detach().cpu().numpy())\n",
    "        lr_plot.append(cur_rl)\n",
    "\n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), lifts)\n",
    "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=175\n",
    "# optimize_shape_as_pierre(model, MESH_TO_OPTIMIZE, verbose=verbose, constraint_rad=0.1, num_iters=num_iters,\n",
    "#                                  save_to_dir='Expirements/OptimizationPaper/AfterMeeting/FreeformDrag/' + str(i), \n",
    "#                                  lr=0.05, adjust_lr_every=20, axis=a, regularization=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Foam Furrier Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_shape_as_pierre_furrier(model, inp, num_params=88, num_iters=100, save_to_dir='Expirements/Optimization',\n",
    "                   lr=0.05, decreased_by=2, adjust_lr_every=10, verbose=None, \n",
    "                                     constraint_rad=0.1, axis=0, regularization=1):\n",
    "\n",
    "    def transform(data, scale):\n",
    "        answ = data.clone()\n",
    "        answ.x = \\\n",
    "            data.x * (scale[1] + scale[2] * data.x)\n",
    "        for k, idx in enumerate(range(3, num_params - 3, 4)):\n",
    "            answ.x += data.x * (scale[idx + 0] * torch.cos((k + 1) * data.x[:, (2, 0, 1)] * 2 * math.pi) + \\\n",
    "                                scale[idx + 1] * torch.cos((k + 1) * data.x[:, (1, 2, 0)] * 2 * math.pi) + \\\n",
    "                                scale[idx + 2] * torch.sin((k + 1) * data.x[:, (2, 0, 1)] * 2 * math.pi) + \\\n",
    "                                scale[idx + 3] * torch.sin((k + 1) * data.x[:, (1, 2, 0)] * 2 * math.pi))\n",
    "        answ.pos = answ.x\n",
    "#         answ.edge_attr = answ.x[ answ.edge_index[0] ] - answ.x[answ.edge_index[1]] \n",
    "        return answ\n",
    "    \n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    model.eval()\n",
    "    data_instance = inp.clone()\n",
    "    data_instance.x.requires_grad = True\n",
    "    scale = torch.zeros((3, num_params), dtype=torch.float).to(\"cuda:0\").t()\n",
    "    scale[1] = 1\n",
    "    scale.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.SGD([scale], lr=lr)\n",
    "\n",
    "    meshes = []\n",
    "    lifts = []\n",
    "    lr_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        transformed_mesh = transform(data_instance, scale)\n",
    "\n",
    "        local_preds = model(transformed_mesh)\n",
    "#             points_in_deepSDF = transformPoints(transformed_mesh.x, np.linalg.inv(AvgTransform))\n",
    "        loss = calculate_loss(transformed_mesh, local_preds, axis=axis, constraint_rad=constraint_rad) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # Regularization\n",
    "        l2_norm = regularization * torch.mean(scale[2:] ** 2)\n",
    "        l2_norm.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss.detach().cpu().numpy(), ' LR: ', cur_rl)\n",
    "                #plot_points_from_torch(points)\n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, local_preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        np.save(preds_save_path, local_preds.cpu().detach().numpy())\n",
    "\n",
    "        lifts.append(loss.detach().cpu().numpy())\n",
    "        lr_plot.append(cur_rl)\n",
    "\n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), lifts)\n",
    "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize_shape_as_pierre_furrier(model, MESH_TO_OPTIMIZE, verbose=1, \n",
    "#                                  save_to_dir='Expirements/OptimizationPaper/FurrierDragDiff', lr=0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AtlasNet Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelAN = SplineCNN8Residuals(3)\n",
    "# modelAN.load_state_dict(torch.load(\"Expirements/Networks15/AtlasCNN8ResNormMapping_latest.nn\"))\n",
    "# modelAN = modelAN.to(device)\n",
    "# modelAN = modelAN.eval()\n",
    "\n",
    "# sys.path.append(\"/cvlabdata2/home/artem/DeepSDF/atlasnet_retrained\") # '/cvlabdata2/home/artem/DeepSDF/atlasnet/'\n",
    "\n",
    "# import dependencies.dataset_shapenet as dataset_shapenet\n",
    "# from dependencies.model import EncoderDecoder\n",
    "# import dependencies.argument_parser as argument_parser\n",
    "\n",
    "# from easydict import EasyDict\n",
    "# import pdb\n",
    "# import pymesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_shape_atlasNet(model, save_to_dir, latent_idx, \n",
    "                            num_iters=100, lr=5e-3, verbose=None, axis=0, constraint_rad=0.1):\n",
    "    \n",
    "    def make_mesh_from_mesh_atlas(points, faces):\n",
    "    \n",
    "        transformed_points = points#transformPoints(points, AvgTransform)\n",
    "\n",
    "        edges = trimesh.geometry.faces_to_edges(faces)\n",
    "        np_points = transformed_points.clone().cpu().detach().numpy()\n",
    "        edge_attr = [np_points[a] - np_points[b] for a, b in edges]\n",
    "\n",
    "        data = torch_geometric.data.Data(x  = transformed_points, \n",
    "                                         pos= transformed_points, \n",
    "                                         face = torch.tensor(faces, \n",
    "                                                         dtype=torch.long).to('cuda:0').t(),\n",
    "                                         edge_attr=torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "                                         edge_index=torch.tensor(edges, dtype=torch.long).to('cuda:0').t().contiguous())\n",
    "        return data\n",
    "\n",
    "\n",
    "    sys.argv = ['foo']\n",
    "    opt = argument_parser.parser()\n",
    "    torch.cuda.set_device(opt.multi_gpu[0])\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    load pre-trained model\n",
    "    \"\"\"\n",
    "    OUT_PATH = \"/cvlabdata2/home/artem/DeepSDF/atlasnet_retrained/optimizations/\"\n",
    "    MODEL_PATH = \"/cvlabdata2/home/artem/DeepSDF/atlasnet_retrained/trained/network.pth\"\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        opt.device = torch.device(f\"cuda:{opt.multi_gpu[0]}\")\n",
    "    else:\n",
    "        # Run on CPU\n",
    "        opt.device = torch.device(f\"cpu\")\n",
    "\n",
    "    network = EncoderDecoder(opt)\n",
    "    network = nn.DataParallel(network, device_ids=opt.multi_gpu)\n",
    "    # load weights\n",
    "    network.load_state_dict(torch.load(MODEL_PATH, map_location='cuda:0'))\n",
    "    # finally keep only decoder\n",
    "    decoder = nn.DataParallel(network.module.decoder)\n",
    "    # and put in eval mode\n",
    "    decoder.eval()\n",
    "\n",
    "    \"\"\"\n",
    "    load pre-trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # init\n",
    "    latent_filename = os.path.join(\n",
    "        \"/cvlabdata2/home/artem/DeepSDF/atlasnet_retrained/reconstructions/codes\", latent_idx + \".pth\"\n",
    "    )\n",
    "    latent = torch.load(latent_filename).squeeze(0).clone().detach().requires_grad_(True)\n",
    "    latent.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "    \n",
    "    '''\n",
    "    Dealing with pathes\n",
    "    '''\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "        \n",
    "    loss_plot = []\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # create mesh\n",
    "        verts, faces = decoder.module.generate_mesh(latent.unsqueeze(0))\n",
    "        \n",
    "        mesh = make_mesh_from_mesh_atlas(verts, faces)\n",
    "        # predict\n",
    "        local_preds = model(mesh)\n",
    "        loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_plot.append(loss.detach().cpu().numpy())\n",
    "        \n",
    "        if verbose is not None and i % verbose == 0:\n",
    "             print(\"Loss at iter\", i, \":\", loss_plot[-1])\n",
    "\n",
    "        np.save(preds_save_path, local_preds.cpu().detach().numpy())\n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), loss_plot)\n",
    "        mesh_save = get_trimesh_from_torch_geo_with_colors(mesh, local_preds)\n",
    "        mesh_save.export(save_path)\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "#     loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "#     loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
    "#           + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "\n",
    "    #loss += compute_volume(mesh.x[None, :, :], mesh.face.t()[None, :, :])\n",
    "    return loss\n",
    "\n",
    "# optimize_shape_atlasNet(modelAN,'Expirements/OptimizationPaper/AfterMeeting/AtlasNetDrag/22', latent_idx='0022',\n",
    "#                         lr=5e-3, verbose=1,  num_iters=30)\n",
    "# optimize_shape_atlasNet(modelAN,'Expirements/OptimizationPaper/FairLoss/23', latent_idx='0023',\n",
    "#                         lr=5e-3, verbose=1,  num_iters=30)\n",
    "# optimize_shape_atlasNet(modelAN,'Expirements/OptimizationPaper/FairLoss/24', latent_idx='0024',\n",
    "#                         lr=5e-3, verbose=1,  num_iters=30)\n",
    "# optimize_shape_atlasNet(modelAN,'Expirements/OptimizationPaper/FairLoss/25', latent_idx='0025',\n",
    "#                         lr=5e-3, verbose=1,  num_iters=30)\n",
    "# optimize_shape_atlasNet(modelAN,'Expirements/OptimizationPaper/FairLoss/26', latent_idx='0026',\n",
    "#                         lr=5e-3, verbose=1,  num_iters=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSDF Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def method4_to_arbitatry_loss(points, ply_mesh, model, constraint_rad=0.1, axis=0):\n",
    "\n",
    "    initial_dir = points.grad.clone()\n",
    "    points.grad.data.zero_()\n",
    "\n",
    "    mesh = make_mesh_from_points(points, ply_mesh)\n",
    "    #signs = compute_signs_for_loss(mesh, transformPoints(normals, AvgTransform))\n",
    "    local_preds = model(mesh)\n",
    "    loss = calculate_loss(mesh, local_preds, axis=axis, constraint_rad=constraint_rad)\n",
    "    loss.backward()\n",
    "\n",
    "    sign = [-p1.dot(p2) for p1, p2 in zip(initial_dir, points.grad)]\n",
    "    \n",
    "    return sign, loss, local_preds, mesh\n",
    "\n",
    "def optimize_shape_deepSDF(decoder, latent, ref_latent, initial_points=None, num_points=None, \n",
    "                           num_iters=100, point_iters=100,  punch_lr_at_reindex_by=1, num_neignours_constr=10,\n",
    "                           reindex_latent_each=50, reindex_num_iterations=500, reindex_num_samples=100,\n",
    "                           lr=0.2, decreased_by=2, adjust_lr_every=10, alpha_penalty=0.05,\n",
    "                           multiplier_func=method4_to_arbitatry_loss, verbose=None, save_to_dir=None, N=256):\n",
    "\n",
    "    def adjust_learning_rate(\n",
    "        initial_lr, optimizer, num_iterations, decreased_by, adjust_lr_every\n",
    "    ):\n",
    "        lr = initial_lr * ((1 / decreased_by) ** (num_iterations // adjust_lr_every)) \\\n",
    "                        * ((punch_lr_at_reindex_by) ** (num_iterations // reindex_latent_each))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "            \n",
    "        return lr\n",
    "    \n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'meshes')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'meshes'))\n",
    "    if not os.path.exists(os.path.join(save_to_dir, 'predictions')):\n",
    "        os.makedirs(os.path.join(save_to_dir, 'predictions'))\n",
    "\n",
    "    decoder.eval()\n",
    "    latent = latent.clone()\n",
    "    latent.requires_grad = True\n",
    "    optimizer = torch.optim.SGD([latent], lr=lr)\n",
    "\n",
    "    loss_plot = []\n",
    "    latent_dist = []\n",
    "    lr_plot = []\n",
    "\n",
    "    if initial_points is not None:\n",
    "        points = initial_points.clone()\n",
    "    else:\n",
    "        points = get_points_from_latent(decoder, latent, N=N, point_num=num_points)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        time_start = time.time()\n",
    "\n",
    "        save_path = os.path.join(save_to_dir, 'meshes/' + str(i).zfill(5) + \".ply\")\n",
    "        preds_save_path = os.path.join(save_to_dir, 'predictions/' + str(i).zfill(5) + \".npy\")\n",
    "\n",
    "#             if i > 0 and i == reindex_latent_each:\n",
    "#                 new_latent = get_latent_from_mesh(decoder, latent_size=latent.size()[1], \n",
    "#                                                   num_iterations=reindex_num_iterations, \n",
    "#                                                   num_samples=reindex_num_samples)\n",
    "#                 latent = torch.Tensor(new_latent.cpu().detach().numpy()).cuda()\n",
    "#                 latent.requires_grad = True\n",
    "#                 optimizer = torch.optim.Adam([latent], lr=lr)\n",
    "\n",
    "        cur_rl = adjust_learning_rate(lr, optimizer, i, decreased_by, adjust_lr_every)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ply_mesh = create_mesh( decoder,\n",
    "                                    latent,\n",
    "                                    N=N,\n",
    "                                    max_batch=int(2 ** 18),\n",
    "                                    offset=None,\n",
    "                                    scale=None)\n",
    "\n",
    "        points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                                    ply_mesh['vertex']['y'][:, None], \n",
    "                                                    ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "        points.requires_grad = True\n",
    "\n",
    "        sdf_value = deep_sdf.utils.decode_sdf(decoder, latent, points)\n",
    "        sdf_value.backward(torch.ones([len(points), 1], dtype=torch.float32).cuda())\n",
    "\n",
    "        mults, loss_value, preds, transformed_mesh = multiplier_func(points, ply_mesh)         \n",
    "        multipliers = torch.cuda.FloatTensor(mults)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        sdf_value = torch.squeeze(deep_sdf.utils.decode_sdf(decoder, latent, points))\n",
    "\n",
    "        final_loss = torch.sum(sdf_value * multipliers)\n",
    "        final_loss.backward()\n",
    "#             print( \"Latent size: \", torch.sum(latent ** 2) )\n",
    "#             print( \"Latent grad: \", torch.sum(latent.grad ** 2) )\n",
    "\n",
    "#           sdf_value.backward(multipliers)\n",
    "\n",
    "\n",
    "        # Soft-constraints\n",
    "        distances, indeces = LATENT_KD_TREE.query(latent.cpu().detach(), k=num_neignours_constr)\n",
    "        penalty = torch.mean(\n",
    "                    torch.stack([torch.sum( \n",
    "                                    (latent - latent_vectors[indeces[0][i]]) ** 2\n",
    "                                 )\n",
    "                                 for i in range(len(indeces[0]))]\n",
    "                               )\n",
    "                    )\n",
    "        apenalty = penalty * alpha_penalty\n",
    "        apenalty.backward()\n",
    "        #print(\"Latent grad penalized: \", torch.sum(latent.grad ** 2))\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Hard-constraints\n",
    "#             if (torch.sum(latent ** 2) > 1.2):\n",
    "#                 latent *= 1.2 / torch.sum(latent ** 2)\n",
    "\n",
    "#             loss_value, preds = loss_func(points, ply_mesh)            \n",
    "\n",
    "        tri_mesh = get_trimesh_from_torch_geo_with_colors(transformed_mesh, preds)\n",
    "        tri_mesh.export(save_path)\n",
    "        np.save(preds_save_path, preds.cpu().detach().numpy())\n",
    "\n",
    "        if save_to_dir is not None:\n",
    "            plot_points_from_torch\n",
    "\n",
    "        loss_plot.append(loss_value.cpu().detach().numpy())\n",
    "        latent_dist.append(torch.sum((latent - ref_latent) ** 2 ).cpu().detach().numpy() )\n",
    "        lr_plot.append(penalty)\n",
    "\n",
    "        time_end = time.time()\n",
    "\n",
    "        if verbose is not None and i % verbose == 0:\n",
    "            print('Iter ', i, 'Loss: ', loss_value.detach().cpu().numpy(), ' LD: ', lr_plot[-1])\n",
    "\n",
    "        np.save(os.path.join(save_to_dir, \"loss_plot.npy\"), loss_plot)\n",
    "        np.save(os.path.join(save_to_dir, \"latent_dist.npy\"), latent_dist)\n",
    "        np.save(os.path.join(save_to_dir, \"lr_plot.npy\"), lr_plot)\n",
    "        np.save(os.path.join(save_to_dir, \"latent%d.npy\" % i), latent.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "def make_full_transformation(initial_latent, ref_latent, experiment_name, \n",
    "                             decoder, model, alpha_penalty=0.05, constraint_rad=0.1, axis=0, **kwargs):\n",
    "    '''\n",
    "    kwargs:\n",
    "        num_iters=1000, \n",
    "        adjust_lr_every=10, \n",
    "        decreased_by=1.2,\n",
    "        lr=0.005,\n",
    "        \n",
    "        reindex_latent_each=100,\n",
    "        punch_lr_at_reindex_by=1,\n",
    "        reindex_num_iterations=500, \n",
    "        reindex_num_samples=100,\n",
    "        \n",
    "        verbose=10,\n",
    "    '''\n",
    "\n",
    "    #ref_points = get_points_from_latent(decoder, ref_latent, N=128)\n",
    "\n",
    "    save_to_dir = experiment_name\n",
    "    if not os.path.exists(save_to_dir):\n",
    "        os.makedirs(save_to_dir)\n",
    "\n",
    "    #np.save(os.path.join(save_to_dir, \"target_verts.npy\"), ref_points)\n",
    "\n",
    "    optimize_shape_deepSDF(decoder, initial_latent, ref_latent, initial_points=None,\n",
    "                                           alpha_penalty=alpha_penalty,\n",
    "                                           num_points=None, point_iters=2,\n",
    "                                           multiplier_func=lambda x, y: \n",
    "                                               method4_to_arbitatry_loss(x, y, model, \n",
    "                                                                         constraint_rad=constraint_rad, \n",
    "                                                                         axis=axis),\n",
    "                                           save_to_dir=save_to_dir, **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_volume(vertices_in, faces_in):\n",
    "    bs, nv = vertices_in.shape[:2]\n",
    "    bs, nf = faces_in.shape[:2]\n",
    "    device = vertices_in.device\n",
    "    faces = faces_in + (torch.arange(bs, dtype=torch.int32).to(device) * nv)[:, None, None]\n",
    "    vertices = vertices_in.reshape((bs * nv, 3))\n",
    "    # pytorch only supports long and byte tensors for indexing\n",
    "    face_coordinates = vertices[faces.long()].squeeze(0)\n",
    "\n",
    "    x1y2z3 = face_coordinates[:,0,0]*face_coordinates[:,1,1]*face_coordinates[:,2,2]\n",
    "    x2y1z3 = face_coordinates[:,1,0]*face_coordinates[:,0,1]*face_coordinates[:,2,2]\n",
    "    x3y1z2 = face_coordinates[:,2,0]*face_coordinates[:,0,1]*face_coordinates[:,1,2]\n",
    "    x1y3z2 = face_coordinates[:,0,0]*face_coordinates[:,2,1]*face_coordinates[:,1,2]\n",
    "    x2y3z1 = face_coordinates[:,1,0]*face_coordinates[:,2,1]*face_coordinates[:,0,2]\n",
    "    x3y2z1 = face_coordinates[:,2,0]*face_coordinates[:,1,1]*face_coordinates[:,0,2]\n",
    "\n",
    "    V = x1y2z3 - x2y1z3 + x3y1z2 - x1y3z2 + x2y3z1 - x3y2z1\n",
    "    ans = 1.0/6.0*torch.abs(torch.sum(V))\n",
    "    print(ans)\n",
    "    return ans\n",
    "\n",
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  -(1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += 10 * boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(-0.1, 0.05, 0))\n",
    "    loss += innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(-0.3, 0, 0))\n",
    "\n",
    "    #loss += compute_volume(mesh.x[None, :, :], mesh.face.t()[None, :, :])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting shape  535\n",
      "Iter  0 Loss:  -0.05752294  LR:  0.2\n",
      "Iter  1 Loss:  -0.06605638  LR:  0.2\n",
      "Iter  2 Loss:  -0.07459407  LR:  0.2\n",
      "Iter  3 Loss:  -0.08316308  LR:  0.2\n",
      "Iter  4 Loss:  -0.0918005  LR:  0.2\n",
      "Iter  5 Loss:  -0.10054854  LR:  0.2\n",
      "Iter  6 Loss:  -0.10945869  LR:  0.2\n",
      "Iter  7 Loss:  -0.11860336  LR:  0.2\n",
      "Iter  8 Loss:  -0.12808554  LR:  0.2\n",
      "Iter  9 Loss:  -0.13805544  LR:  0.2\n",
      "Iter  10 Loss:  -0.14875007  LR:  0.2\n",
      "Iter  11 Loss:  -0.1605454  LR:  0.2\n",
      "Iter  12 Loss:  -0.17405614  LR:  0.2\n",
      "Iter  13 Loss:  -0.19030702  LR:  0.2\n",
      "Iter  14 Loss:  -0.21103828  LR:  0.2\n",
      "Iter  15 Loss:  -0.23926103  LR:  0.2\n",
      "Iter  16 Loss:  -0.28027478  LR:  0.2\n",
      "Iter  17 Loss:  -0.3435629  LR:  0.2\n",
      "Iter  18 Loss:  -0.44643754  LR:  0.2\n",
      "Iter  19 Loss:  -0.620728  LR:  0.2\n",
      "Iter  20 Loss:  -0.9251458  LR:  0.1\n",
      "Iter  21 Loss:  -1.176847  LR:  0.1\n",
      "Iter  22 Loss:  -1.5219252  LR:  0.1\n",
      "Iter  23 Loss:  -2.0004792  LR:  0.1\n",
      "Iter  24 Loss:  -2.6652339  LR:  0.1\n",
      "Iter  25 Loss:  -3.5955687  LR:  0.1\n",
      "Iter  26 Loss:  -4.919287  LR:  0.1\n",
      "Iter  27 Loss:  -6.824587  LR:  0.1\n",
      "Iter  28 Loss:  -9.597604  LR:  0.1\n",
      "Iter  29 Loss:  -13.645423  LR:  0.1\n",
      "Iter  30 Loss:  -19.683384  LR:  0.1\n",
      "Iter  31 Loss:  -28.88633  LR:  0.1\n",
      "Iter  32 Loss:  -43.216854  LR:  0.1\n",
      "Iter  33 Loss:  -66.45885  LR:  0.1\n",
      "Iter  34 Loss:  -104.44983  LR:  0.1\n",
      "Iter  35 Loss:  -170.47723  LR:  0.1\n",
      "Iter  36 Loss:  -297.5255  LR:  0.1\n",
      "Iter  37 Loss:  -585.4755  LR:  0.1\n",
      "Iter  38 Loss:  -1394.3907  LR:  0.1\n",
      "Iter  39 Loss:  -4533.877  LR:  0.1\n",
      "Starting shape  69\n",
      "Iter  0 Loss:  -0.1170984  LR:  0.2\n",
      "Iter  1 Loss:  -0.12570019  LR:  0.2\n",
      "Iter  2 Loss:  -0.1342803  LR:  0.2\n",
      "Iter  3 Loss:  -0.14287534  LR:  0.2\n",
      "Iter  4 Loss:  -0.1515263  LR:  0.2\n",
      "Iter  5 Loss:  -0.16027704  LR:  0.2\n",
      "Iter  6 Loss:  -0.16918586  LR:  0.2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b7097631eb76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m                                  \u001b[0msave_to_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expirements/OptimizationPaper/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexperiment_dir\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                              \u001b[0;34m'/Vertex'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                  lr=0.2, adjust_lr_every=20, axis=a)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-95d38c3d7a7b>\u001b[0m in \u001b[0;36moptimize_shape_by_vertex\u001b[0;34m(model, inp, num_iters, save_to_dir, lr, decreased_by, adjust_lr_every, verbose, constraint_rad, axis)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m#print('Avg grad: ', torch.mean(torch.sum(data_instance.x.grad ** 2, axis=1)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtri_mesh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trimesh_from_torch_geo_with_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtri_mesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_save_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvlabdata2/home/artem/DeepSDF/custom_utils.py\u001b[0m in \u001b[0;36mget_trimesh_from_torch_geo_with_colors\u001b[0;34m(mesh, preds, vmin, vmax)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     return trimesh.Trimesh(vertices=verticies, faces=faces, \n\u001b[0;32m--> 276\u001b[0;31m                            vertex_colors=list(map(lambda c: m.to_rgba(c),  preds[:, 0].cpu().detach())))\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvlabdata2/home/artem/DeepSDF/custom_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(c)\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     return trimesh.Trimesh(vertices=verticies, faces=faces, \n\u001b[0;32m--> 276\u001b[0;31m                            vertex_colors=list(map(lambda c: m.to_rgba(c),  preds[:, 0].cpu().detach())))\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cm.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, value, clip)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0mresdat\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mresdat\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresdat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36mmask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3452\u001b[0m         \u001b[0;31m# Return a view so that the dtype and shape cannot be changed in place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3453\u001b[0m         \u001b[0;31m# This still preserves nomask by identity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3454\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3456\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment_dir = 'AfterMeeting'\n",
    "num_iters = 40\n",
    "N = 256\n",
    "verbose = 30\n",
    "\n",
    "spoilers_detach = [246]\n",
    "spoilers_sportives = [64, 95, 118]\n",
    "nice_shapes = [175, 185, 113, 69, 61, 49, 64]\n",
    "\n",
    "nice_shapes2 = [61, 69, 113, 185]\n",
    "for i in [535, 69, 32, 162, 61]:\n",
    "    LATENT_TO_OPTIMIZE = latent_vectors[i]\n",
    "    LATENT_KD_TREE = KDTree(np.array([lv.cpu().detach().numpy()[0] for lv in latent_vectors]))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ply_mesh = create_mesh( decoder,\n",
    "                                LATENT_TO_OPTIMIZE,\n",
    "                                N=N,\n",
    "                                max_batch=int(2 ** 18),\n",
    "                                offset=None,\n",
    "                                scale=None)\n",
    "\n",
    "    points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "    normals = torch.cuda.FloatTensor(np.hstack(( ply_mesh['normals']['x'][:, None], \n",
    "                                                 ply_mesh['normals']['y'][:, None], \n",
    "                                                 ply_mesh['normals']['z'][:, None])))\n",
    "\n",
    "    MESH_TO_OPTIMIZE = make_mesh_from_points(points, ply_mesh)\n",
    "    #SAVED_SIGNS = compute_signs_for_loss(MESH_TO_OPTIMIZE, transformPoints(normals, AvgTransform))\n",
    "\n",
    "    print(\"Starting shape \", i)\n",
    "    \n",
    "    for a, name in zip([0], ['Drag']):\n",
    "#         DeepSDF\n",
    "#         make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "#                                     'Expirements/OptimizationPaper/' + experiment_dir + \n",
    "#                                          '/DeepSDF' + name + 'constr/' + str(i), decoder, model,\n",
    "#                          alpha_penalty=0.2, axis=a,\n",
    "#                          num_iters=num_iters,\n",
    "#                          adjust_lr_every=20,\n",
    "#                          decreased_by=1.1, \n",
    "#                          lr=0.2,\n",
    "#                          verbose=None,\n",
    "#                          N=N,\n",
    "#                          num_neignours_constr=10,\n",
    "#                          reindex_latent_each=10000,\n",
    "#                          punch_lr_at_reindex_by=1,\n",
    "#                          reindex_num_iterations=500,\n",
    "#                          reindex_num_samples=100)\n",
    "\n",
    "#         FreeFormFurrier\n",
    "#         optimize_shape_as_pierre_furrier(model, MESH_TO_OPTIMIZE, verbose=verbose, \n",
    "#                                          constraint_rad=0.1, num_iters=num_iters,\n",
    "#                                  save_to_dir='Expirements/OptimizationPaper/' + experiment_dir + \n",
    "#                                              '/FreeformFurrie' + name + '/' + str(i), \n",
    "#                                  lr=0.005, adjust_lr_every=20, axis=a, regularization=1)\n",
    "\n",
    "        # FreeForm\n",
    "#         optimize_shape_as_pierre(model, MESH_TO_OPTIMIZE, verbose=verbose, constraint_rad=0.1, num_iters=num_iters,\n",
    "#                                  save_to_dir='Expirements/OptimizationPaper/' + experiment_dir + \n",
    "#                                              '/Freeform' + name + '/' + str(i) + 'minus', \n",
    "#                                  lr=0.1, adjust_lr_every=20, axis=a, regularization=0)\n",
    "\n",
    "#         # Scaling\n",
    "#         optimize_shape_by_scaling(model, MESH_TO_OPTIMIZE, verbose=1, constraint_rad=0.1, num_iters=num_iters,\n",
    "#                                  save_to_dir='Expirements/OptimizationPaper/' + experiment_dir + \n",
    "#                                              '/Scaling' + name + '/' + str(i), \n",
    "#                                  lr=0.5, adjust_lr_every=10, axis=a)\n",
    "\n",
    "        # Vertex\n",
    "        optimize_shape_by_vertex(model, MESH_TO_OPTIMIZE, verbose=1, constraint_rad=0.1, num_iters=num_iters,\n",
    "                                 save_to_dir='Expirements/OptimizationPaper/' + experiment_dir + \n",
    "                                             '/Vertex' + name + '/' + str(i), \n",
    "                                 lr=0.2, adjust_lr_every=20, axis=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Optimizations for the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dir = 'TruckVisualizations'\n",
    "LATENT_TO_OPTIMIZE = latent_vectors[32]\n",
    "LATENT_KD_TREE = KDTree(np.array([lv.cpu().detach().numpy()[0] for lv in latent_vectors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(-0.05, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/cabin05', decoder, model,\n",
    "                         alpha_penalty=0.2, axis=0,\n",
    "                         constraint_rad=0.05,\n",
    "                         num_iters=30,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=None,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(-0.15, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/cabinBack15', decoder, model,\n",
    "                         alpha_penalty=0.2, axis=0,\n",
    "                         constraint_rad=0.2,\n",
    "                         num_iters=30,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=1,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(-0.15, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/cabinFront15', decoder, model,\n",
    "                         alpha_penalty=0.2, axis=0,\n",
    "                         constraint_rad=0.1,\n",
    "                         num_iters=30,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=1,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "#     loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "#     loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(0.05, 0.05, 0))  \\\n",
    "#           + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/axis0.2', decoder, model,\n",
    "                         alpha_penalty=0.3, axis=0.2,\n",
    "                         constraint_rad=0.1,\n",
    "                         num_iters=100,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=None,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "#     loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "#     loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(0.05, 0.05, 0))  \\\n",
    "#           + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/axis0.5', decoder, model,\n",
    "                         alpha_penalty=0.3, axis=0.2,\n",
    "                         constraint_rad=0.1,\n",
    "                         num_iters=100,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=None,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "#     loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "#     loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(0.05, 0.05, 0))  \\\n",
    "#           + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/axis0.8', decoder, model,\n",
    "                         alpha_penalty=0.3, axis=0.8,\n",
    "                         constraint_rad=0.1,\n",
    "                         num_iters=100,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=None,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(mesh, local_preds, axis=0, constraint_rad=0.1):\n",
    "    loss =  (1 - axis) * compute_lift_faces_diff(mesh, local_preds, axis=0) + \\\n",
    "                  axis * compute_lift_faces_diff(mesh, local_preds, axis=1)\n",
    "    \n",
    "    loss += boundsLoss(mesh.x, box=[(-0.6, 0.6, 0)])\n",
    "    loss += innerBoundsLoss(mesh.x, r=constraint_rad**2, center=(-0.15, 0.05, 0))  \\\n",
    "          + innerBoundsLoss(mesh.x, r=(constraint_rad / 2)**2, center=(0.3, 0, 0))\n",
    "    return loss\n",
    "\n",
    "make_full_transformation(LATENT_TO_OPTIMIZE.detach(), LATENT_TO_OPTIMIZE.clone(), \n",
    "                                    'Expirements/OptimizationPaper/' + experiment_dir + '/32/cabinBackSmall15', decoder, model,\n",
    "                         alpha_penalty=0.2, axis=0,\n",
    "                         constraint_rad=0.05,\n",
    "                         num_iters=30,\n",
    "                         adjust_lr_every=20,\n",
    "                         decreased_by=1.1, \n",
    "                         lr=0.2,\n",
    "                         verbose=None,\n",
    "                         N=256,\n",
    "                         num_neignours_constr=10,\n",
    "                         reindex_latent_each=10000,\n",
    "                         punch_lr_at_reindex_by=1,\n",
    "                         reindex_num_iterations=500,\n",
    "                         reindex_num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "my_data = genfromtxt('../Expirements/SavedTransforms/DeepSDF-CFD8-m3-holes/report/00000/output/cloud_p_k_omega_nut.csv', \n",
    "                     delimiter=',', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load(\"../Data/cars_remeshed_dsdf/inputs/0_input.stl\")\n",
    "norm = mpl.colors.Normalize(vmin= -2, vmax=2)\n",
    "cmap = cm.hot\n",
    "m = cm.ScalarMappable(norm=norm, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld_tree = KDTree(my_data[:, :3])\n",
    "distances, indeces = fld_tree.query(mesh.vertices, k=1)\n",
    "\n",
    "interpolations = my_data[:, 0][indeces].squeeze()\n",
    "mesh = trimesh.Trimesh(vertices=mesh.vertices, faces=mesh.faces, \n",
    "                           vertex_colors=list(map(lambda c: m.to_rgba(c),  interpolations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load(\"../Data/cars_refined/simulated/stl/0005.stl\")\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = make_data_instance_from_stl(\"/cvlabdata2/home/artem/Data/check_simulations/Processed/fld/0002_0005.fld\")\n",
    "di.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = get_trimesh_from_torch_geo_with_colors(di, di.y)\n",
    "lift = compute_lift_faces(di, di.y)\n",
    "print('Lift: ', lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(di.y[:, 0].detach().cpu().numpy(), bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/check_simulations/Processed/fld/0002_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/check_simulations/Processed/fld/0000_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/check_simulations/Processed/fld/0001_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/cars_refined/simulated/fld/0002_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/cars_refined/simulated/fld/0003_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/cars_refined/simulated/fld/0007_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/check_simulations/ListenerCheck/processed/fld/0000_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/check_simulations/ListenerCheck/processed/fld/0001_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "npar = np.genfromtxt(\"/cvlabdata2/home/artem/Data/check_simulations/ListenerCheck/processed/fld/0002_0005.fld\", delimiter=',', skip_header=1)\n",
    "plt.hist(npar[:, 3], bins=100, alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di0 = make_data_instance_from_stl(\"../Expirements/SimulatedResults/foam_npy/fld/0000_0005.fld\")\n",
    "di1 = make_data_instance_from_stl(\"../Expirements/SimulatedResults/foam_npy/fld/0001_0005.fld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(di0.y[:, 0], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(di1.y[:, 0], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.load('/cvlabdata2/home/artem/Data/cars_refined/simulated/stl/0005.stl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Expirements/SavedTransforms/DeepSDF-CFD8-m3-holes/meshes/00000.ply'\n",
    "tri_mesh = trimesh.load(path)\n",
    "points = torch.tensor(tri_mesh.vertices, dtype=torch.float).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "innerBoundsLoss(points, r=0.12**2, center=(0.55, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "bvecs = genfromtxt('examples/cars/vine_latentVecs_tll_0.1_1.csv', delimiter=' ', skip_header=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bvecs = [torch.FloatTensor(v).to('cuda:0') for v in bvecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ply_mesh = create_mesh( decoder,\n",
    "                        latent_vectors[0],\n",
    "                        N=128,\n",
    "                        max_batch=int(2 ** 18),\n",
    "                        offset=None,\n",
    "                        scale=None)\n",
    "\n",
    "points = torch.cuda.FloatTensor(\n",
    "    np.array([ply_mesh['vertex']['x'], ply_mesh['vertex']['y'], ply_mesh['vertex']['z']]).transpose()\n",
    ")\n",
    "\n",
    "transformed_points = make_mesh_from_points(points, ply_mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(transformed_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = get_trimesh_from_torch_geo_with_colors(transformed_points, pred.cpu().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = mesh.export('examples/cars/BeautifiedMeshes/o0.ply')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_latent_from_mesh_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = trimesh.geometry.faces_to_edges(ply_mesh['face']['vertex_indices'])\n",
    "edge_attr = torch.stack([points[a] - points[b] for a, b in edges])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(edge_attr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PyntCloud(pd.DataFrame(points.cpu().detach().numpy(), columns=['x', 'y', 'z']))\n",
    "cloud.plot(background='white', initial_point_size=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PyntCloud(pd.DataFrame(points_transformed.cpu().detach().numpy(), columns=['x', 'y', 'z']))\n",
    "cloud.plot(background='white', initial_point_size=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = make_data_instance_from_stl('/cvlabdata2/home/artem/Data/cars_refined/simulated/fld/0007_0005.fld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_trimesh_from_torch_geo_with_colors(di, di.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PyntCloud(pd.DataFrame(di.pos.cpu().detach().numpy(), columns=['x', 'y', 'z']))\n",
    "cloud.plot(background='white', initial_point_size=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PyntCloud(pd.DataFrame(transformed_points.pos.cpu().detach().numpy(), columns=['x', 'y', 'z']))\n",
    "cloud.plot(background='white', initial_point_size=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    ply_mesh = create_mesh( decoder,\n",
    "                            latent,\n",
    "                            N=128,\n",
    "                            max_batch=int(2 ** 18),\n",
    "                            offset=None,\n",
    "                            scale=None)\n",
    "\n",
    "points = torch.cuda.FloatTensor(np.hstack(( ply_mesh['vertex']['x'][:, None], \n",
    "                                            ply_mesh['vertex']['y'][:, None], \n",
    "                                            ply_mesh['vertex']['z'][:, None])))\n",
    "\n",
    "mesh = trimesh.Trimesh(vertices=points.cpu().detach(), faces=ply_mesh['face']['vertex_indices'])\n",
    "edge_attr = [mesh.vertices[a] - mesh.vertices[b] for a, b in mesh.edges]\n",
    "\n",
    "data = torch_geometric.data.Data(x  = points, \n",
    "                                 pos= torch.tensor(mesh.vertices, dtype=torch.float).to('cuda:0'), \n",
    "                                 faces = torch.tensor(mesh.faces, dtype=torch.long).to('cuda:0'),\n",
    "                                 edge_attr = torch.tensor(edge_attr, dtype=torch.float).to('cuda:0'),\n",
    "                                 edge_index= torch.tensor(mesh.edges, dtype=torch.long).t().contiguous().to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_instance= make_data_instance_from_stl(\n",
    "                    '/cvlabsrc1/cvlab/dataset_shapenet/code/foam_npy/fld/0100_0005.fld', data_step=1)\n",
    "mesh = trimesh.Trimesh(vertices=data_instance.pos, faces=data_instance.faces)\n",
    "a = mesh.export('../Expirements/data/original_mesh.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expirements with PreprocessMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent = get_latent_from_mesh_cpu(decoder, latent_size, mesh, num_iterations=300, num_samples=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_gpu = get_latent_from_mesh(decoder, latent_size, mesh_path='../Expirements/data/original_mesh.ply', \n",
    "                         num_iterations=300, num_samples=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_points = deep_sdf.data.read_sdf_samples_into_ram('../Expirements/data/original_SDF.npz')\n",
    "gpu_points[1][:, 3].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PyntCloud(pd.DataFrame(np.array(gpu_points[0][:, :3]), columns=['x', 'y', 'z']))\n",
    "cloud.plot(background='white', initial_point_size=0.03, elev=-45, azim=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, sdf = sample_sdf_near_surface(mesh)\n",
    "sdfs = np.hstack((points, sdf[:, None]))\n",
    "data_sdf = [torch.from_numpy(sdfs[sdfs[:, 3] > 0, :]), \n",
    "            torch.from_numpy(sdfs[sdfs[:, 3] < 0, :])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_sdf[0].shape)\n",
    "print(gpu_points[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = PyntCloud(pd.DataFrame(np.array(data_sdf[0][:, :3]), columns=['x', 'y', 'z']))\n",
    "cloud.plot(background='white', initial_point_size=0.03, elev=-45, azim=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mesh_from_vector(decoder, latent, N=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plydata = create_mesh(decoder, latent_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plydata.write(\"../Expirements/mesh.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
